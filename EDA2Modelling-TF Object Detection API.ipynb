{"cells":[{"metadata":{"_cell_guid":"0d3335f2-7e95-41e9-9b11-024b441f20a8","_uuid":"33b1c6ee129e7fcd9b2a85b72fece42ead34ad6a"},"cell_type":"markdown","source":"Hello Kagglers!! Enjoying life and spending time on Kaggle? Well, since last few days, I am not feeling well at all but my addiction to Kaggle is totally different. Even though I do so much of data science and ML/DL in my daily life but if I don't visit Kaggle or our slack,  [KaggleNoobs](https://kagglenoobs.herokuapp.com/), the work doesn't seem to be complete. There was a discussion in that slack over Tensorflow Object Detection API and I agree that to a newcomer, it might be slightly overwhelming. But once you get it, it's the most straight forward thing to apply to your work and achieve amazing results before you try something that isn't out there.  Yes, you guessed it right, today's kernel is going to be a walkthrough for the TF object detection API and I hope you will like it. \n\n![detection](https://media.giphy.com/media/StRnSltcS0n04/giphy.gif)\n\n**PS:** There are certain constraints in the kernel, regarding the installation of some ubuntu packages that are required to be installed for using the API but be assured that even though there is no straightforward way to use it in kernels, you will find this tutorial very very helpful to speed up your modeling. So, let's dive in."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport glob\nimport cv2\nimport numba as nb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nfrom time import time\nfrom numba import jit\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom concurrent.futures import ProcessPoolExecutor\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom skimage.io import imread\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(111)\ncolor = sns.color_palette()\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Defining some paths as usual\ninput_dir = Path('../input/')\ndata_dir = input_dir / 'data_300x300/data_300x300'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5421cf76-536a-4ee3-902f-fe61eb5f803e","_uuid":"676d75c81296fb305d4f2f0856708ba3bd311d8d"},"cell_type":"markdown","source":"Quoting from the data description: `This dataset contains images and labels of feline reticulocytes (an immature red blood cell without a nucleus, having a granular or reticulated appearance when suitably stained). The dataset was created using equipment that is easily accessible to veterinarians: a standard laboratory microscope and two types of cameras: a basic microscope camera and a smartphone camera`\n\nLet's look how the dataset is arranged"},{"metadata":{"_cell_guid":"51dad4c9-d7e0-4b81-b465-fad38f6e4f57","_uuid":"937d3e27548047fab7c025fbf22e9618e370a3dd","trusted":true},"cell_type":"code","source":"os.listdir(data_dir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af179a31-ddf8-49b5-8ad3-0d1d0a949da4","_uuid":"35c7f5a7c3c59cb7f76737b732c7b2273549f0be","collapsed":true},"cell_type":"markdown","source":"The `images` directory contains the images for training,  the `labels` directory contains the corresponding annotations for the training images and `TEST` contains the test images. How are the annotations done? When you annotate any object with a bounding box, there are certain things that you need to take care of in the annotations. The annotation corresponding to an image should contain the coordinates of the bounding boxes, the height of the image, the width of the image and the label corresponding to that box.  Here is an example of the annotation in our dataset:\n\n```\n<annotation>\n\t<folder>images</folder>\n\t<filename>000045.jpg</filename>\n\t<path>/home/vini/Desktop/data_300x300/images/000045.jpg</path>\n\t<source>\n\t\t<database>Unknown</database>\n\t</source>\n\t<size>\n\t\t<width>300</width>\n\t\t<height>300</height>\n\t\t<depth>3</depth>\n\t</size>\n\t<segmented>0</segmented>\n\t<object>\n\t\t<name>aggregate reticulocyte</name>\n\t\t<pose>Unspecified</pose>\n\t\t<truncated>0</truncated>\n\t\t<difficult>0</difficult>\n\t\t<bndbox>\n\t\t\t<xmin>140</xmin>\n\t\t\t<ymin>115</ymin>\n\t\t\t<xmax>169</xmax>\n\t\t\t<ymax>143</ymax>\n\t\t</bndbox>\n\t</object>\n\t<object>\n\t\t<name>punctate reticulocyte</name>\n\t\t<pose>Unspecified</pose>\n\t\t<truncated>0</truncated>\n\t\t<difficult>0</difficult>\n\t\t<bndbox>\n\t\t\t<xmin>72</xmin>\n\t\t\t<ymin>155</ymin>\n\t\t\t<xmax>103</xmax>\n\t\t\t<ymax>187</ymax>\n\t\t</bndbox>\n\t</object>\n\t<object>\n\t\t<name>erythrocyte</name>\n\t\t<pose>Unspecified</pose>\n\t\t<truncated>0</truncated>\n\t\t<difficult>0</difficult>\n\t\t<bndbox>\n\t\t\t<xmin>184</xmin>\n\t\t\t<ymin>195</ymin>\n\t\t\t<xmax>213</xmax>\n\t\t\t<ymax>228</ymax>\n\t\t</bndbox>\n\t</object>\n</annotation>\n```\n\nYou can see the height and width of the image, the bounding box `bndbox`, the coordinates of the bounding box `xmin, ymin, xmax, ymax`, the label corresponding to that bounding box  given by the node `name` . There are a lot of opensource tools that you can use for annotating datasets but amongst all of them, the simplest and the best one is [labelImg](https://github.com/tzutalin/labelImg). "},{"metadata":{"_cell_guid":"12a33c4f-17e8-4dae-852e-f2133f1e768f","_uuid":"edf16cfae8d346d7c65dab731644377578d7f11d"},"cell_type":"markdown","source":"## Preprocessing\n\nThe annotations are given as `xmls`. The Tensorflow Object detection API accepts data in `TFRecords` format. So, we need to process our annotations"},{"metadata":{"_cell_guid":"2f007a46-f597-42a5-9f34-15c636d9a5a6","_uuid":"2090c9d9f91352619776e695d74537fa177c415e","collapsed":true,"trusted":true},"cell_type":"code","source":"# A function to parse the xmls\ndef parse_xmls(xml_files):\n    data = []\n    # Iterate over each file\n    for sample in xml_files:\n        # Get the xml tree\n        tree = ET.parse(sample)\n\n        # Get the root\n        root = tree.getroot()\n\n        # Get the members and extract the values\n        for member in root.findall('object'):\n            # Name of the image file\n            filename = root.find('filename').text\n            \n            # Height and width of the image\n            width =  int((root.find('size')).find('width').text)\n            height = int((root.find('size')).find('height').text)\n            \n            # Bounding box coordinates\n            bndbox = member.find('bndbox')\n            xmin = float(bndbox.find('xmin').text)\n            xmax = float(bndbox.find('xmax').text)\n            ymin = float(bndbox.find('ymin').text)\n            ymax = float(bndbox.find('ymax').text)\n            \n            # label to the corresponding bounding box\n            label =  member.find('name').text\n\n            data.append((filename, width, height, label, xmin, ymin, xmax, ymax))\n    \n    # Create a pandas dataframe\n    columns_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n    df = pd.DataFrame(data=data, columns=columns_name)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d107587c-f507-4457-a405-79e059d76587","_uuid":"c3a2d9af6fef86241072c74c8df05d3638a08e96","trusted":true},"cell_type":"code","source":"images = sorted(glob.glob('../input/data_300x300/data_300x300/images/*.jpg'))\nxmls = sorted(glob.glob('../input/data_300x300/data_300x300/labels/*.xml'))\nprint(\"Total number of images: \", len(images))\nprint(\"Total number of xmls: \", len(xmls))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68c77d45-53f3-417a-b9a0-3fd5b3a97ffc","_uuid":"196e55582ec8fba77bef68e4ae18b0c8ef31cfe0","trusted":true},"cell_type":"code","source":"# Parse the xmls and get the data in a dataframe\ndf = parse_xmls(xmls)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f4bdd08-bb85-4c5d-b43b-2b7e6d4b5535","_uuid":"d085d18f0f377a3256fd8862397a264903afff26","trusted":true},"cell_type":"code","source":"# How many classes do we have for object detection?\nlabel_counts = df['class'].value_counts()\nprint(label_counts)\n\nplt.figure(figsize=(20,8))\nsns.barplot(x=label_counts.index, y= label_counts.values, color=color[2])\nplt.title('Labels in our dataset', fontsize=14)\nplt.xlabel('Label', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(label_counts.index)), ['erythrocyte', 'punctate reticulocyte', 'aggregate reticulocyte'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db1561a5-9d7c-44c1-89f6-4d56f2ab0f0f","_uuid":"9b5df8f1ce86e434b384edfa9d936b27153d1e37","trusted":true},"cell_type":"code","source":"train, valid = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=111)\n\ntrain = train.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)\nprint(\"Number of training samples: \", len(train))\nprint(\"Number of validation samples: \", len(valid))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e02702ba-62ee-4964-b3b6-12d9b84b874d","_uuid":"829d00d5287b37131bd6030ae4ecf89979c838c6"},"cell_type":"markdown","source":"# The TensorFlow Object Detection API setup\n\nAs I said there are certains things that needs to be installed on the host computer for using the API which is not possible in kernels, so I will demonstrate the steps in markdown. \n\n1.  Install tensorflow-gpu. Make sure you have installed the right version of Cuda and cuDNN. For more information, click [here](https://www.tensorflow.org/install/)\n2. Make a directory where you want to store all of the work and just cd into it\n3. Clone the tensorflow models repo `git clone https://github.com/tensorflow/models.git`\n4.  Install protobuf compiler `sudo apt-get install protobuf-compiler`\n5. Install other dependencies:\n    * pip install Cython\n    * pip install pillow\n    * pip install lxml\n    * pip install jupyter\n    * pip install matplotlib\n \n6.  `cd models/research/`\n\n7.   `protoc object_detection/protos/*.proto --python_out=.`\n\n8.   ```export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim```\n\n9.   Test your installation: `python object_detection/builders/model_builder_test.py`\n\nIf the last step ran successfully then, you are done with the set up. Yay!!!"},{"metadata":{"_cell_guid":"3c1123d2-455a-4471-ae16-6139ce31f673","_uuid":"98ceb24fdba9905bcf1cc6482d1639128b221024"},"cell_type":"markdown","source":"# LabelMap\n\nBefore converting you dataset to TFRecords, you need to make sure that you have a labelmap corresponding to all the labels that are in your dataset. For our dataset, the labelmap looks like this:\n\n```\nitem {\n  id: 1\n  name: 'erythrocyte'\n}\n\nitem {\n  id: 2\n  name: 'punctate reticulocyte'\n}\n\nitem {\n  id: 3\n  name: 'aggregate reticulocyte'\n}\n\n```\n\n**Note**: Numbering starts from 1 as 0 is treated as background. I have named this labelmap as `bloodmap.pbtxt` and at this point my work directory looks like this:\n\n```\n/home\n     /ubuntu\n           /Nain\n                 /models\n                     /research\n                            /blood_train\n                                   /felina\n                                        /data_300x300\n                             bloodmap.pbtxt\n```\n\n`blood_train` is the directory that I created for this project."},{"metadata":{"_cell_guid":"cf62b777-4ba9-4c3f-b421-9f75fdb2e63d","_uuid":"6f64b5661207c9a87ce8f8bccbcac2f6a322c33c"},"cell_type":"markdown","source":"# Converting your data to TFRecords format\n\nThis is the part where most of the the beginners get stuck. They have no clue how to do this. But this is quite simple. You have all your data information stored in the dataframe. The most importnat thing to remember is that a single image can contain multiple labels(bounding boxes) in the annotations, so you have to do a `groupby` on your dataframe. Let's see how we can do that.\n\n```python\n# Import the packages required\nimport sys\nsys.path.append(\"..\")\nimport io\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nfrom PIL import Image\nfrom collections import namedtuple, OrderedDict\nfrom models.research.object_detection.utils import dataset_util\nfrom models.research.object_detection.utils import label_map_util\n\n# Function to group data and return the same\n# Group by imagefile name\ndef make_groups(df, field=None):\n    if field==None:\n        field = 'filename'\n        \n    data = namedtuple('object', ['filename', 'info'])\n    grouped = df.groupby(field)\n    \n    grouped_data = []\n    for filename, x in zip(grouped.groups.keys(), grouped.groups):\n        grouped_data.append(data(filename, grouped.get_group(x)))\n        \n    return grouped_data\n    \n    \n  # Creating a tf record sample\n  def create_tf_example(group, img_path, label_map_dict)\n      # Read the imagefile. This will be used in features later \n      with tf.gfile.GFile(os.path.join(img_path, '{}'.format(group.filename)), 'rb') as f:\n          img_file = f.read()\n    \n      # Encode to bytes and read using PIL. Could be done directly too\n      encoded_img = io.BytesIO(img_file)\n      # Read the image using PIL\n      img = Image.open(encoded_img)\n      width, height = img.size\n    \n      # Encode the name of the img file\n      filename = group.filename.encode('utf8')\n      \n      # Define the format of the image file\n      img_format = b'jpg'   # The name will be in bytes\n    \n    \n      # Define the variables that you need as features\n      xmins = []\n      xmaxs = []\n      ymins = []\n      ymaxs = []\n      classes_text = []\n      classes = []\n\n      # Iterate over the namedtuple object\n      for index, row in group.info.iterrows():\n          xmins.append(row['xmin'] / width)   # store normalized values for bbox\n          xmaxs.append(row['xmax'] / width)\n          ymins.append(row['ymin'] / height)\n          ymaxs.append(row['ymax'] / height)\n          classes_text.append(row['class'].encode('utf8'))\n          classes.append(label_map_dict[row['class']])\n\n      tf_example = tf.train.Example(features=tf.train.Features(feature={\n          'image/height': dataset_util.int64_feature(height),\n          'image/width': dataset_util.int64_feature(width),\n          'image/filename': dataset_util.bytes_feature(filename),\n          'image/source_id': dataset_util.bytes_feature(filename),\n          'image/encoded': dataset_util.bytes_feature(img_file),\n          'image/format': dataset_util.bytes_feature(img_format),\n          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n          'image/object/class/label': dataset_util.int64_list_feature(classes),}))\n    \n      return tf_example\n```\n\n"},{"metadata":{"_cell_guid":"294a3c5c-d290-4e41-ac5f-5ef1b19b1cb9","_uuid":"1731015af1999959db56b47fb7f1006c9cff23db"},"cell_type":"markdown","source":"Great!! We have defined all the functions required for preprocessing and all. While creating TFRecords, all we need to do is to open a TFRecords writer instance and create `train.record` and `valid.record` from our `train` and `valid` dataframes. Let's do that.\n\n```python\n# Path where all the images are present\nimg_path = './felina/data_300x300/images/'\n# Label map\nlabel_map_dict = label_map_util.get_label_map_dict('./bloodmap.pbtxt')\n\nwriter = tf.python_io.TFRecordWriter('./train.record')\n# create groups in the df. One image may contain several instances of an object hence the grouping thing\nimg_groups = make_groups(train, field='filename')\n# Iterate over the samples in each group create a TFRecord\nfor group in img_groups:\n    tf_example = create_tf_example(group, img_path, label_map_dict)\n    writer.write(tf_example.SerializeToString())\n# close the writer\nwriter.close()\nprint(\"TFRecords for training data  created successfully\")\n\n\nwriter = tf.python_io.TFRecordWriter('./valid.record')\n# create groups \nimg_groups = make_groups(valid, field='filename')\n# Iterate over the samples in each group create a TFRecord\nfor group in img_groups:\n    tf_example = create_tf_example(group, img_path, label_map_dict)\n    writer.write(tf_example.SerializeToString())\n# close the writer\nwriter.close()\nprint(\"TFRecords for validation data created successfully\")\n```"},{"metadata":{"_cell_guid":"89548f1a-422c-457d-9dd5-d7fc4e6cd98b","_uuid":"bb3d2e5d4e3108c41deb93a075a183f961e926c5"},"cell_type":"markdown","source":"# Adding the model you want to use \n\nNow we have almost everything ready.  We need to do two more steps:\n* Choosing the model config file that you want to use for training\n* Downloading the weights of the same model from [TensorFlow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\n\nLet's do this.\n1.  I chose the ssd_inception_v2 model for my training but you can chose whichever you like. `cp models/research/object_detection/samples/configs/ssd_inception_v2_coco.config`\n\n2. `wget download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz`\n\n3.  `unzip ssd_inception_v2_coco_2017_11_17.tar.gz`\n\n4. `mv ssd_inception_v2_coco_2017_11_17 ssd_inceptionv2`\n\nNow your work directory should be like this:\n\n```\n/home\n     /ubuntu\n           /Nain\n                 /models\n                     /research\n                            /blood_train\n                                /felina\n                                    /data_300x300\n                                 /ssd_inceptionv2      \n                                 bloodmap.pbtxt\n                                 train.record\n                                 valid.record\n                                 ssd_inception_v2_coco.config\n```"},{"metadata":{"_cell_guid":"2a374161-ef15-46cb-8ae1-64dbd4bcc346","_uuid":"11037f5b3b37462524982919722e019f43adef80"},"cell_type":"markdown","source":"# Configuring your model config file\n\nIn the model config that you chose to use, you need to make some changes. You need to give the path of the `tfrecords` and the `labelmap` files as well as the checkpoint of that model for fine tuning. Open your config file and edit the following lines:\n\n\n```\nnum_classes: 3\n\n\nfine_tune_checkpoint:\"/home/ubuntu/Nain/models/research/blood_train/ssd_inceptionv2/model.ckpt\"\n\ntrain_input_reader: {\n  tf_record_input_reader {\n    input_path: \"/home/ubuntu/Nain/models/research/blood_train/train.record\"\n  }\n  label_map_path: \"/home/ubuntu/Nain/models/research/blood_train/bloodmap.pbtxt\"\n}\n\neval_input_reader: {\n  tf_record_input_reader {\n    input_path: \"/home/ubuntu/Nain/models/research/blood_train/valid.record\"\n  }\n  label_map_path:\"/home/ubuntu/Nain/models/research/blood_train/bloodmap.pbtxt\"\n  shuffle: false\n  num_readers: 1\n}\n\n```"},{"metadata":{"_cell_guid":"5a98ea33-9eb6-4992-9545-cb45583cb1ef","_uuid":"19c33cc0fad2d4d06d4baa3662180475cf8eff73"},"cell_type":"markdown","source":"# Training\n\nWe are almost done!! I know it's too much in one go but once you do it, you will become very comfortable in using it. To start the training, we need to do two things:\n* Create a directory for storing training checkpoints. I named it `checkpoints`\n* Copy the `train.py`,  `eval.py` and  `export_inference_graph.py` from the `object_detection` directory to our current directory\n\nThis is how your things should be arranged by now:\n\n```\n/home\n     /ubuntu\n           /Nain\n                 /models\n                     /research\n                            /blood_train\n                                /felina\n                                    /data_300x300\n                                 /ssd_inceptionv2\n                                 /checkpoints\n                                 bloodmap.pbtxt\n                                 train.record\n                                 valid.record\n                                 ssd_inception_v2_coco.config\n                                 train.py\n                                 eval.py\n                                 export_inference_graph.py\n```\n\nAnd the final command to run the training!!!!\n\n```\npython train.py --logtostderr --train_dir=/home/ubuntu/Nain/models/research/blood_train/checkpoints/ --pipeline_config_path=/home/ubuntu/Nain/models/research/blood_train/ssd_inception_v2_coco.config\n\n```\n\n"},{"metadata":{"_cell_guid":"f1862aa7-0182-4498-8636-e4e7b87d1b0e","_uuid":"cfff0da8174c0bcbcbfd8a83f7c1f04074a5e037"},"cell_type":"markdown","source":"# Freezing the graph\n\nOnce you are done with the training, you need to freeze the graph for doing inference.  The checkpoint depends on the number of iterations you completed for training.  I completed 25K iterations for this, but you should do more as it's not enough.  Freeze the graph:\n\n```\npython export_inference_graph.py --input_type image_tensor --pipeline_config_path /home/ubuntu/Nain/models/research/blood_train/ssd_inception_v2_coco.config --trained_checkpoint_prefix ./checkpoints/model.ckpt-25823 --output_directory ./fine_tuned_model\n```"},{"metadata":{"_cell_guid":"dca028ec-48a9-40a5-ab9d-a120c589e0bf","_uuid":"27bb53a32e27498b838ee29cf05c2d7aef49f113"},"cell_type":"markdown","source":"These are the results that I got on some sample images\n\n![](https://i.imgur.com/TpRvjvp.png)\n\n![](https://i.imgur.com/E4DwYEG.png)"},{"metadata":{"_cell_guid":"e535cbad-4edc-4fee-ad31-543339340ceb","_uuid":"db0f710e0e0c934c2b0c60db59eaa854607af212"},"cell_type":"markdown","source":"And that's it folks!! I have tried my best to give you an overview of the TF object detection API in a most simplified way.  The motivation behind this kernel actually came from the discussion  regarding the TF API in our [KaggleNoobs](https://kagglenoobs.herokuapp.com/) slack. **Please upvote if this you liked this kernel**. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}